{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyabf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are a set of convenience functions for working with Homeira and Lihua's files \n",
    "\n",
    "\n",
    "# this function tries to guess the gain on the response (voltage) channel\n",
    "# the logic is to compare the voltage value at time 0 and compare that to the listed RMP\n",
    "# the best gain minimizes that difference\n",
    "CHECK_RESPONSE_GAINS = np.array([.25, .33, .5, 1, 2, 20, 25, 50]) # these are the valid gains that Homeira /Lihua uses\n",
    "def guess_response_gain(resp_vec, stated_rmp):\n",
    "    # this function is a bit of logic that tries to guess the gain given an rmp value\n",
    "    # try to figure out gain on response channel by comparing to RMP\n",
    "\n",
    "    abs_diff_vec = np.abs(resp_vec[0] * CHECK_RESPONSE_GAINS - cell_rmp)\n",
    "    best_gain_ind = np.argmin(abs_diff_vec)\n",
    "    \n",
    "    rmp_abs_error = abs_diff_vec[best_gain_ind]\n",
    "    best_gain = CHECK_RESPONSE_GAINS[best_gain_ind]\n",
    "    return(best_gain, rmp_abs_error)\n",
    "\n",
    "# try to figure out the gain on the stimulus channel\n",
    "def get_stim_gain(stim_vec):\n",
    "    min_stim = np.min(stim_vec)\n",
    "    if min_stim > -1:\n",
    "        stim_gain = 1000\n",
    "    else:\n",
    "        stim_gain = 1\n",
    "    return stim_gain\n",
    "\n",
    "# parse relevant info related to stimulus, including duration, and amplitudes\n",
    "def get_stim_info(abf, stim_channel_num = 1, stim_gain = 1):\n",
    "    num_sweeps = abf.sweepCount\n",
    "    stim_amps = np.zeros(num_sweeps) \n",
    "    stim_start_time = None\n",
    "    stim_end_time = None\n",
    "    for i in range(0, num_sweeps):\n",
    "        abf.setSweep(i, channel=stim_channel_num)\n",
    "        sampling_rate = abf.dataRate\n",
    "        stim_vec = abf.sweepC * stim_gain\n",
    "        stim_amp = stim_vec[5000]\n",
    "\n",
    "        stim_amps[i] = round(stim_amp)\n",
    "        non_zero_inds = np.where(stim_vec != 0)\n",
    "        stim_duration = np.shape(non_zero_inds)[1] * 1/sampling_rate\n",
    "        if stim_duration == 0:\n",
    "            continue\n",
    "        stim_start_ind = non_zero_inds[0][0]\n",
    "        stim_end_ind = non_zero_inds[0][-1]\n",
    "        \n",
    "        stim_start_time = abf.sweepX[stim_start_ind]\n",
    "        stim_end_time = abf.sweepX[stim_end_ind]\n",
    "    sampling_rate = int(round(1/(abf.sweepX[2] - abf.sweepX[1]))) # manually calculate the sampling rate\n",
    "\n",
    "    ret_dict = {'stim_amp_vec' : stim_amps, 'stim_duration' : stim_duration, \n",
    "                'stim_start_time' : stim_start_time, 'stim_end_time' : stim_end_time, 'num_sweeps' : num_sweeps,\n",
    "               'stim_sampling_rate' : sampling_rate}\n",
    "    return(ret_dict)\n",
    "\n",
    "# gets all relevant info about stimulus, including channel, duration, etc. and returns as dictionary\n",
    "def get_stim_dict(meta_row, cell_meta_df):\n",
    "    # returns path of abf file containing stim info\n",
    "    # stim channel index\n",
    "    # stim gain\n",
    "    # other info like num sweeps, which \n",
    "    \n",
    "    row = meta_row\n",
    "    f = row['cell_id'].values[0]\n",
    "    fn = row['full_path'].values[0]\n",
    "    recorder_name = row['recorder_name'].values[0]\n",
    "    abf = pyabf.ABF(fn) # loads in the abf file\n",
    "\n",
    "    stim_info_dict = {}    \n",
    "\n",
    "    # figure out stim channel\n",
    "    stim_chan = len(abf.channelList)-1 # this seems to be generally true\n",
    "    abf.setSweep(0, channel=stim_chan)\n",
    "    stim_vec = abf.sweepC\n",
    "    stim_gain = get_stim_gain(stim_vec)\n",
    "\n",
    "    # this infers some basic info about stim amplitudes, durations, etc.\n",
    "    stim_info_dict = get_stim_info(abf, stim_chan, stim_gain = stim_gain)\n",
    "    stim_amps = stim_info_dict['stim_amp_vec']\n",
    "    sampling_rate = stim_info_dict['stim_sampling_rate']\n",
    "    if np.std(stim_amps) == 0 and recorder_name == 'Homeira':\n",
    "        stim_chan = 0\n",
    "        abf.setSweep(0, channel=stim_chan)\n",
    "        stim_vec = abf.sweepC\n",
    "        stim_gain = get_stim_gain(stim_vec)\n",
    "        print(stim_gain)\n",
    "        stim_info_dict = get_stim_info(abf, stim_chan, stim_gain = stim_gain)\n",
    "    elif np.std(stim_amps) == 0 and recorder_name == 'Lihua' and num_sweeps == 30:\n",
    "        # logic here is that if abf file meets these criteria, we should replace the stimulus with the one from\n",
    "        # a specific abf file with available info\n",
    "        abf_file_name = '14617300.abf'\n",
    "        curr_row = cell_meta_df.loc[cell_meta_df['cell_id'] == abf_file_name]\n",
    "        row = curr_row\n",
    "        return get_stim_dict(row, cell_meta_df) # woo recursion\n",
    "    stim_amps = stim_info_dict['stim_amp_vec']\n",
    "    if np.std(stim_amps) == 0:\n",
    "        valid_stim = False\n",
    "    else:\n",
    "        valid_stim = True\n",
    "    ret_dict = {'stim_chan' : stim_chan, 'stim_gain' : stim_gain, 'stim_path' : fn, 'valid_stim' : valid_stim}\n",
    "    \n",
    "    ret_dict.update(stim_info_dict)\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in cell metadata and processed ephys property dictionary\n",
    "# this is needed because we need to know what the RMP values are per cell\n",
    "\n",
    "summary_table_csv = '/Users/stripathy/rstudio_projects/valiante_ih/summary_tables/all_cells.csv'\n",
    "#excel_file = file_base_base_path + 'valiante_lab_ephys_mar_2020/L23/Homeira/Total2Homeira-Lastversion_March 10_2020.xlsx'\n",
    "cell_info = pd.read_csv(open(summary_table_csv, 'rb'))  \n",
    "len(cell_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories containing abf files from Homeira and Lihua's experiments\n",
    "\n",
    "### change the directory paths below to where the files are on your local directory\n",
    "file_base_base_path = '/Users/stripathy/Downloads/'\n",
    "file_base_path = 'valiante_lab_ephys_mar_2020/'\n",
    "\n",
    "path = file_base_base_path + file_base_path\n",
    "\n",
    "# get all abf files in directories\n",
    "files = [f for f in glob.glob(path + \"**/*.abf\", recursive=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19122022.abf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.basename(files[0]))\n",
    "\n",
    "# traverse directories and start keeping track of important metadata\n",
    "files_list = list()\n",
    "for f in files:\n",
    "    base_file_name = os.path.basename(f)\n",
    "    ret_dict = {'cell_id' : base_file_name, 'full_path' : f}\n",
    "    files_list.append(ret_dict)\n",
    "    \n",
    "file_dict = pd.DataFrame(files_list)\n",
    "len(file_dict)\n",
    "\n",
    "# drop file duplicates\n",
    "file_dict = file_dict.drop_duplicates(subset=['cell_id'])\n",
    "len(file_dict)\n",
    "#file_dict.sort_values(by = ['abf_version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the metadata and file info pandas data frames\n",
    "cell_meta_df = cell_info.merge(file_dict, on='cell_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stim_chan': 0,\n",
       " 'stim_gain': 1,\n",
       " 'stim_path': '/Users/stripathy/Downloads/valiante_lab_ephys_mar_2020/L5/Lihua/TotalL5-Lihua-Lastversion/14617300.abf',\n",
       " 'valid_stim': True,\n",
       " 'stim_amp_vec': array([-400., -375., -350., ...,  275.,  300.,  325.]),\n",
       " 'stim_duration': 0.6,\n",
       " 'stim_start_time': 0.0656,\n",
       " 'stim_end_time': 0.66556,\n",
       " 'num_sweeps': 30,\n",
       " 'stim_sampling_rate': 25000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the usage of the convience functions defined above on a single abf file\n",
    "\n",
    "abf_file_name = '14617300.abf'\n",
    "curr_row = cell_meta_df.loc[cell_meta_df['cell_id'] == abf_file_name]\n",
    "get_stim_dict(curr_row, cell_meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_11_28_0079.abf\n",
      "2019_11_28_0093.abf\n",
      "2020_01_06_0017.abf\n",
      "2020_01_06_0048.abf\n",
      "2020_01_06_0063.abf\n",
      "2020_01_06_0082.abf\n",
      "2020_01_06_0090.abf\n",
      "2020_01_06_0095.abf\n",
      "2020_01_27_0038.abf\n",
      "19122022.abf\n",
      "skipping file because too many abf channels\n",
      "19122024.abf\n",
      "skipping file because too many abf channels\n",
      "19129040.abf\n",
      "skipping file because too many abf channels\n",
      "19129047.abf\n",
      "skipping file because too many abf channels\n",
      "19129043.abf\n",
      "skipping file because too many abf channels\n",
      "2015_11_09_0003.abf\n",
      "2015_11_09_0017.abf\n",
      "2015_11_09_0053.abf\n",
      "2015_11_09_0085.abf\n",
      "2015_11_09_0106.abf\n",
      "2015_11_09_0107.abf\n",
      "2016_02_25_0005.abf\n",
      "2016_02_25_0007.abf\n",
      "2016_02_25_0073.abf\n",
      "2016_02_25_0082.abf\n",
      "2016_02_25_0134.abf\n",
      "2016_02_25_0255.abf\n",
      "2016_03_01_0000.abf\n",
      "2016_03_01_0047.abf\n",
      "2016_03_03_0002.abf\n",
      "2016_03_03_0054.abf\n",
      "2016_03_03_0100.abf\n",
      "2016_03_03_0103.abf\n",
      "19122017.abf\n",
      "skipping file because too many abf channels\n",
      "19129057.abf\n",
      "skipping file because too many abf channels\n",
      "19129069.abf\n",
      "skipping file because too many abf channels\n",
      "19320024.abf\n",
      "skipping file because too many abf channels\n",
      "19320025.abf\n",
      "skipping file because too many abf channels\n",
      "19320030.abf\n",
      "skipping file because too many abf channels\n",
      "13d02004.abf\n",
      "13d02022.abf\n",
      "13d02049.abf\n",
      "13d03005.abf\n",
      "13d03007.abf\n",
      "13d03008.abf\n",
      "13d03029.abf\n",
      "13d03040.abf\n",
      "13n05011.abf\n",
      "13n21007.abf\n",
      "15616008.abf\n",
      "15616022.abf\n",
      "15616035.abf\n",
      "15618005.abf\n",
      "15622000.abf\n",
      "15622005.abf\n",
      "15622013.abf\n",
      "15622015.abf\n",
      "15622019.abf\n",
      "15622024.abf\n",
      "15723013.abf\n",
      "15820003.abf\n",
      "15o08002.abf\n",
      "15o08007.abf\n",
      "15o08011.abf\n",
      "15o08017.abf\n",
      "15o08020.abf\n",
      "15o08022.abf\n",
      "15o08032.abf\n",
      "15o08038.abf\n",
      "2015_11_09_0078.abf\n",
      "20131211_600_1_0002.abf\n",
      "1000\n",
      "20131211_600_1_0042.abf\n",
      "1000\n",
      "20131211_600_1_0066.abf\n",
      "1000\n",
      "20131211_600_1_0067.abf\n",
      "1000\n",
      "20131211_600_1_0081.abf\n",
      "1000\n",
      "20131211_600_1_0108.abf\n",
      "1000\n",
      "20131211_600_1_0160.abf\n",
      "1000\n",
      "20131211_600_1_0184.abf\n",
      "1000\n",
      "20131211_600_1_0190.abf\n",
      "1000\n",
      "20140127_600_1_0118.abf\n",
      "1000\n",
      "20140127_600_1_0158.abf\n",
      "1000\n",
      "2016_01_28_0004.abf\n",
      "2016_01_28_0012.abf\n",
      "2016_01_28_0019.abf\n",
      "2016_02_04_0009.abf\n",
      "2016_02_04_0015.abf\n",
      "2016_02_04_0018.abf\n",
      "2016_02_04_0021.abf\n",
      "2016_02_04_0029.abf\n",
      "2016_02_04_0033.abf\n",
      "2016_02_04_0042.abf\n",
      "2016_02_04_0045.abf\n",
      "2016_02_29_0032.abf\n",
      "2016_02_29_0065.abf\n",
      "19122026.abf\n",
      "skipping file because too many abf channels\n",
      "19128003.abf\n",
      "skipping file because too many abf channels\n",
      "19128036.abf\n",
      "skipping file because too many abf channels\n",
      "19128037.abf\n",
      "skipping file because too many abf channels\n",
      "19128040.abf\n",
      "skipping file because too many abf channels\n",
      "19128044.abf\n",
      "skipping file because too many abf channels\n",
      "19128061.abf\n",
      "skipping file because too many abf channels\n",
      "19128065.abf\n",
      "skipping file because too many abf channels\n",
      "19128068.abf\n",
      "skipping file because too many abf channels\n",
      "19129004.abf\n",
      "skipping file because too many abf channels\n",
      "19129015.abf\n",
      "skipping file because too many abf channels\n",
      "19129022.abf\n",
      "skipping file because too many abf channels\n",
      "19129024.abf\n",
      "skipping file because too many abf channels\n",
      "19129037.abf\n",
      "skipping file because too many abf channels\n",
      "19320001.abf\n",
      "skipping file because too many abf channels\n",
      "19320007.abf\n",
      "skipping file because too many abf channels\n",
      "19320041.abf\n",
      "skipping file because too many abf channels\n",
      "2019_11_04_0113.abf\n",
      "2019_11_26_0006.abf\n",
      "2019_11_26_0094.abf\n",
      "2019_11_26_0110.abf\n",
      "2019_11_28_0119.abf\n",
      "2020_01_27_0002.abf\n",
      "2020_01_27_0008.abf\n",
      "2020_01_27_0042.abf\n",
      "2020_01_28_0008.abf\n",
      "2020_01_28_0017.abf\n",
      "2020_01_28_0029.abf\n",
      "2020_03_02_0023.abf\n",
      "14424000.abf\n",
      "14424329.abf\n",
      "14424335.abf\n",
      "14515300.abf\n",
      "14515323.abf\n",
      "14515349.abf\n",
      "14515372.abf\n",
      "14515374.abf\n",
      "14515424.abf\n",
      "14520300.abf\n",
      "14520332.abf\n",
      "14605300.abf\n",
      "14605321.abf\n",
      "14605325.abf\n",
      "14605338.abf\n",
      "14605349.abf\n",
      "14605374.abf\n",
      "14617300.abf\n",
      "14617312.abf\n",
      "14624300.abf\n",
      "14624311.abf\n",
      "14624325.abf\n",
      "14624359.abf\n",
      "14624371.abf\n",
      "14626300.abf\n",
      "14626311.abf\n",
      "14715300.abf\n",
      "14715312.abf\n",
      "14715334.abf\n",
      "14911303.abf\n",
      "14911315.abf\n",
      "14918300.abf\n",
      "14918302.abf\n",
      "14918344.abf\n",
      "14918372.abf\n",
      "14n03302.abf\n",
      "14n03328.abf\n",
      "14n04322.abf\n",
      "14n04346.abf\n",
      "14n04362.abf\n",
      "14n10302.abf\n",
      "14n10459.abf\n",
      "15224000.abf\n",
      "15224059.abf\n",
      "15302001.abf\n",
      "15330000.abf\n",
      "15330053.abf\n",
      "15407000.abf\n",
      "15420000.abf\n",
      "15420026.abf\n",
      "14304300.abf\n",
      "14304311.abf\n",
      "14317300.abf\n",
      "14317320.abf\n",
      "14317348.abf\n",
      "14318308.abf\n",
      "14408304.abf\n",
      "14d02000.abf\n",
      "14d02055.abf\n",
      "14d02065.abf\n",
      "14d02124.abf\n",
      "14d16000.abf\n",
      "14d16016.abf\n",
      "14d16022.abf\n",
      "14d16056.abf\n",
      "14d16134.abf\n",
      "14d18005.abf\n",
      "14d18033.abf\n",
      "14d18087.abf\n",
      "14d18113.abf\n",
      "15105000.abf\n",
      "15105065.abf\n",
      "15105080.abf\n",
      "15105107.abf\n",
      "15127001.abf\n",
      "15127028.abf\n",
      "15127041.abf\n",
      "15127061.abf\n",
      "15127092.abf\n",
      "15127106.abf\n",
      "15127132.abf\n",
      "15312000.abf\n",
      "15312031.abf\n",
      "15312053.abf\n",
      "15312105.abf\n",
      "15312132.abf\n"
     ]
    }
   ],
   "source": [
    "# sweep through all listed abf files and extract all relevant metadata required to parse raw data\n",
    "\n",
    "path = file_base_base_path + file_base_path\n",
    "\n",
    "files = [f for f in glob.glob(path + \"**/*.abf\", recursive=True)]\n",
    "\n",
    "#lihua_stim_abf = \n",
    "dict_list = list()\n",
    "\n",
    "for abf_file in cell_meta_df.cell_id:\n",
    "    print(abf_file)\n",
    "    row = cell_meta_df.loc[cell_meta_df['cell_id'] == abf_file]\n",
    "    \n",
    "    f = row['cell_id'].values[0]\n",
    "    #\n",
    "    fn = row['full_path'].values[0]\n",
    "    #print(fn)\n",
    "    resp_chan = 0\n",
    "    stim_chan = 0\n",
    "    stim_info_dict = {}\n",
    "    \n",
    "    cell_rmp = row['rmp'].values[0]\n",
    "    recorder_name = row['recorder_name'].values[0]\n",
    "\n",
    "    \n",
    "    abf = pyabf.ABF(fn) # loads in the abf file\n",
    "    num_sweeps = abf.sweepCount\n",
    "    #print(recorder_name)\n",
    "    #print(num_sweeps)\n",
    "    \n",
    "    if len(abf.channelList) > 2:\n",
    "        # don't know what to do with these, skipping for now\n",
    "        print('skipping file because too many abf channels')\n",
    "        \n",
    "        ret_dict = {'cell_id' : f, 'abf_version' : abf.abfVersionString, \n",
    "                    'valid_stim': False\n",
    "                   }\n",
    "        dict_list.append(ret_dict)\n",
    "        \n",
    "        continue\n",
    "        stim_chan = 3\n",
    "        abf.setSweep(0, channel=stim_chan)\n",
    "        stim_vec = abf.sweepY\n",
    "        \n",
    "        resp_chan = 2\n",
    "        abf.setSweep(0, channel=resp_chan)\n",
    "        resp_vec = abf.sweepY\n",
    "        \n",
    "        stim_gain = .2\n",
    "        \n",
    "    else:\n",
    "\n",
    "        # figure out response channel\n",
    "        abf.setSweep(0, channel=resp_chan)\n",
    "        resp_vec = abf.sweepY\n",
    "        resp_sampling_rate = int(round(1/(abf.sweepX[2] - abf.sweepX[1]))) # manually calculate the sampling rate\n",
    "\n",
    "        # try to figure out gain on response channel by comparing to RMP\n",
    "        (best_gain, rmp_abs_error) = guess_response_gain(resp_vec, cell_rmp)\n",
    "\n",
    "        stim_dict = get_stim_dict(row, cell_meta_df)\n",
    "        \n",
    "    \n",
    "    ret_dict = {'cell_id' : f, 'resp_chan' : resp_chan, \n",
    "                    'resp_gain' : best_gain, 'rmp_error' : rmp_abs_error, \n",
    "                'rmp_val' : cell_rmp,\n",
    "                'abf_version' : abf.abfVersionString, 'resp_sampling_rate' : resp_sampling_rate}\n",
    "    #ret_dict = dict(ret_dict.items() + stim_info_dict.items())\n",
    "    ret_dict.update(stim_dict)\n",
    "    #print(ret_dict)\n",
    "\n",
    "    dict_list.append(ret_dict)\n",
    "    \n",
    "\n",
    "cell_raw_file_meta_df = pd.DataFrame(dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge together cell_raw_file_meta_df with some other cell metadata from the imported csv\n",
    "\n",
    "cell_meta_df_small = cell_meta_df[['cell_id', 'expt_date', 'layer_name', 'recorder_name', 'full_path']]\n",
    "cell_final_raw_meta_df = cell_meta_df_small.merge(cell_raw_file_meta_df, on='cell_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_final_raw_meta_df = cell_final_raw_meta_df.sort_values(by = ['expt_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a final csv that has the ouput of this metadata gathering process\n",
    "csv_meta_save_path = 'output_files/cell_final_raw_meta_df.csv'\n",
    "cell_final_raw_meta_df.to_csv(csv_meta_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
